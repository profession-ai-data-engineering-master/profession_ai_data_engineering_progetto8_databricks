{
  "metadata": {
    "name": "profession_ai_data_engineering_progetto8_local",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Analisi di Wikipedia\n\n## Descrizione del Progetto\n\n**Wikidata Insights**, un\u0027azienda leader nella gestione di contenuti digitali, è stata incaricata da **Wikipedia** per ottimizzare l\u0027analisi e la categorizzazione dei contenuti di Wikipedia.\nPer supportare la loro continua espansione e migliorare l\u0027organizzazione delle informazioni, Wikidata Insights ha deciso di condurre un progetto avanzato di **data analysis e machine learning**.\nL\u0027obiettivo principale è comprendere meglio il vasto patrimonio di contenuti informativi offerti da Wikipedia e sviluppare un sistema di **classificazione automatica** che consenta di categorizzare efficacemente i nuovi articoli futuri.\n\n## Obiettivi\n\n### 1. Analisi Descrittiva dei Contenuti\n\nIl primo obiettivo del progetto è condurre un\u0027**analisi esplorativa dei dati (EDA)** per capire le caratteristiche dei contenuti di Wikipedia suddivisi in diverse categorie tematiche, come ad esempio:\n\n* Cultura\n* Economia\n* Medicina\n* Tecnologia\n* Politica\n* Scienza\n  e altre.\n\nL\u0027analisi esplorativa prevede:\n\n* il **conteggio degli articoli** presenti per ogni categoria.\n* il **numero medio di parole** per articolo.\n* la lunghezza dell\u0027articolo **più lungo** e di quello **più corto** per ciascuna categoria.\n* la creazione di **nuvole di parole** rappresentative per ogni categoria, per identificare i termini più frequenti e rilevanti.\n\n### 2. Sviluppo di un Classificatore Automatico\n\nIl secondo obiettivo è creare un modello di **machine learning** capace di classificare automaticamente gli articoli in base alla loro categoria.\n\nIl sistema di classificazione verrà addestrato utilizzando dati di testo presenti nelle seguenti colonne del dataset:\n\n* **Sommario** (`summary`): Introduzione breve dell\u0027articolo.\n* **Testo Completo** (`documents`): Contenuto completo dell\u0027articolo.\n\n### 3. Identificazione di Nuovi Insights\n\nL\u0027analisi consentirà anche di ottenere preziosi insights sui contenuti di Wikipedia, come la densità di articoli per categoria o le tendenze linguistiche associate a determinati argomenti.\nQueste informazioni possono aiutare Wikimedia a migliorare l\u0027organizzazione delle pagine e a ottimizzare i propri sforzi editoriali.\n\n## Workflow del Progetto\n\n### Caricamento dei Dati\n\nI dati è salvato su S3 e reperibile al seguente link:\n`https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv`\n\nUtilizzando un framework distribuito come **Databricks**, i dati vengono processati in modo efficiente, partendo da un **Pandas DataFrame** per essere successivamente convertiti in un **Spark DataFrame** e salvati come una tabella chiamata `Wikipedia`.\n\nPer caricare il dataset e trasformarlo in una table basta eseguire su Notebook Databricks le seguenti righe di codice:\n\n```python\n!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\nimport pandas as pd\n\ndataset \u003d pd.read_csv(\"/databricks/driver/wikipedia.csv\")\nspark_df \u003d spark.createDataFrame(dataset)\nspark_df \u003d spark_df.drop(\"Unnamed: 0\")\nspark_df.write.saveAsTable(\"wikipedia\")\n```\n\n**N.B.** Durante il loading del dataset, ci appoggiamo ad un dataframe Pandas. Questa non è una procedura comune e del tutto corretta.\nIn questo caso ci permette di leggere correttamente (superando con poco sforzo il limite dei separatori) i dati con cui definire un DataFrame Spark e una Table `Wikipedia`.\n\n## Risultati Attesi\n\n### 1. Ottimizzazione dell\u0027Organizzazione dei Contenuti\n\nL\u0027analisi esplorativa fornirà a Wikimedia una visione chiara e dettagliata della distribuzione e delle caratteristiche dei propri contenuti.\nSarà possibile identificare quali categorie necessitano di maggiore attenzione o dove sono presenti opportunità di espansione.\n\n### 2. Classificazione Automatica\n\nIl sistema di classificazione sviluppato permetterà a Wikimedia di automatizzare il processo di categorizzazione dei nuovi articoli, migliorando l\u0027efficienza operativa e garantendo una migliore navigabilità per gli utenti.\n\n### 3. Nuovi Insights Strategici\n\nGrazie agli strumenti dell\u0027analisi esplorativa e della classificazione permetteranno a Wikimedia di ottimizzare l\u0027allocazione delle risorse editoriali, con la possibilità di orientare le proprie campagne informative in modo più mirato.\n\n## Conclusioni\n\nIl progetto offre a **Wikimedia** un potente strumento di **analisi dei dati** e **classificazione automatica** per migliorare la gestione dei propri contenuti.\nAttraverso l\u0027utilizzo di tecniche avanzate di **data science** e **machine learning**, Wikimedia sarà in grado di ottimizzare la propria infrastruttura informativa e offrire un servizio di qualità superiore agli utenti di tutto il mondo.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "--- "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n\r\n# Premessa\r\n\r\nA causa del recente aggiornamento delle politiche gratuite di Databricks ho riscontrato alcune difficoltà nel completare il compito in cloud, perché:\r\n\r\n* il download del dataset tramite `wget` era limitato a **500 MB**;\r\n* nella versione gratuita di Databricks le librerie **spark.ml** risultano bloccate.\r\n\r\nPer aggirare questi vincoli ho configurato e ottimizzato Apache Zeppelin in locale, svolgendo l’esercizio in quell’ambiente (pur con le relative limitazioni).\r\n\r\nDi seguito trovate:\r\n\r\n1. la **repository pubblica** con la mia configurazione di Zeppelin;\r\n2. la **repository** contenente il compito in formato **`.zpln`**;\r\n3. il **notebook Databricks** sul quale ho copiato e versionato gli stessi contenuti, quindi esportato in **`.ipynb`** come richiesto per l’import in questo notebook Colab.\r\n\r\nEcco i tre link alle repository:\r\n- [Zeppelin Docker Local Tuned](https://github.com/fedevita/zeppelin-docker-local-tuned.git)  \r\n- [Progetto 8 – Zeppelin (Org. personale dedicata al master)](https://github.com/profession-ai-data-engineering-master/profession_ai_data_engineering_progetto8_zeppelin.git)  \r\n- [Progetto 8 – Databricks (Org. personale dedicata al master)](https://github.com/profession-ai-data-engineering-master/profession_ai_data_engineering_progetto8_databricks.git)\r\n\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SETUP"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport os\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql.functions import (\n    avg, \n    max, \n    min, \n    length, \n    count, \n    desc, \n    regexp_replace, \n    col, \n    lower,\n    explode,\n    asc,\n    desc,\n    row_number,\n    struct,\n    map_from_entries,\n    collect_list,\n    concat_ws\n)\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.sql import Window\nfrom pyspark.ml.feature import (\n    StringIndexer,\n    Tokenizer,\n    StopWordsRemover,\n    CountVectorizer,\n    HashingTF,\n    IDF,\n    StandardScaler,\n    PCA\n)\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# da eliminare per databricks\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder \\\n    .getOrCreate()\n\nconf \u003d spark.sparkContext.getConf()\n\nprint(\"App Name:       \", spark.sparkContext.getConf().get(\"spark.app.name\"))\nprint(\"Master:         \", spark.sparkContext.master)\nprint(\"Spark Version:  \", spark.version)\nprint(\"Deploy Mode:    \", spark.sparkContext.getConf().get(\"spark.submit.deployMode\", \"N/D\"))\nprint(\"Application ID: \", spark.sparkContext.applicationId)\nprint(\"Spark Master:          \", conf.get(\"spark.master\"))\nprint(\"Spark Driver Memory:   \", conf.get(\"spark.driver.memory\"))\nprint(\"Spark Executor Memory: \", conf.get(\"spark.executor.memory\"))\nprint(\"Spark Cores:           \", conf.get(\"spark.driver.cores\", \"default\u003d1\"))\nprint(\"Default Parallelism:   \", spark.sparkContext.defaultParallelism)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **O – Obtain (Ottenere i dati)**\n### Obiettivo:\nRecuperare e caricare i dati in un ambiente adatto per l\u0027analisi e la modellazione.\n### Task:\n#### 1. Scaricare il dataset da `http://proai-dataset.s3.eu-west-3.amazonaws.com/wikipedia.csv`"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\r\nwget --progress\u003ddot:mega -O /opt/zeppelin/data/wikipedia.csv https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nwikipedia_raw \u003d (\r\n    spark.read.option(\"header\", True)\r\n              .option(\"multiLine\", True)\r\n              .option(\"quote\", \u0027\"\u0027)\r\n              .option(\"escape\", \u0027\"\u0027)\r\n              .csv(\"/opt/zeppelin/data/wikipedia.csv\")\r\n)\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **S – Scrub (Pulizia e preparazione dei dati)**\n### Obiettivo:\nPulire i dati ed effettuare operazioni preliminari per renderli pronti per l’analisi e il modello."
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 1. Rimuovere righe duplicate o inconsistenti (es. articoli vuoti)\n# 2. Gestire eventuali valori nulli (in title, summary, documents, categoria)\n# 3. Rimuovere o normalizzare caratteri speciali e markup da documents\n\nCLEAN_HTML \u003d \"\u003c[^\u003e]+\u003e\"\nCLEAN_SPECIAL_CHARS \u003d \"[^a-zA-Z0-9\\\\s]\"\n\nwikipedia_clean \u003d (\n    wikipedia_raw\n    .drop(\"_c0\")\n    .dropDuplicates()\n    .dropna(subset\u003d[\"title\", \"summary\", \"documents\", \"categoria\"])\n    .withColumn(\n        \"documents\",\n        lower(regexp_replace(\n            regexp_replace(\n                col(\"documents\"),\n                CLEAN_HTML,\n                \"\"\n            ),\n            CLEAN_SPECIAL_CHARS,\n            \"\"\n        ))\n    )\n)\nwikipedia_clean.persist(StorageLevel.DISK_ONLY)\nwikipedia_clean.count()\nwikipedia_clean.createOrReplaceTempView(\"wikipedia_clean\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * from wikipedia_clean limit 3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **E – Explore (Analisi esplorativa dei dati)**\n\n### Obiettivo:\n\nCapire la struttura, la distribuzione e le peculiarità dei dati per categoria.\n\n### Task:"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 1. Calcolare il **conteggio articoli per categoria**"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n_ \u003d (wikipedia_clean\n.groupBy(\"categoria\")\n.agg(\n    count(\"*\").alias(\u0027documents_cnt\u0027)\n)\n.sort(desc(\"documents_cnt\"))\n.createOrReplaceTempView(\"wikipedia_e1\")\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT * FROM wikipedia_e1\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 2. Calcolare la **lunghezza media, minima e massima** per articolo per categoria"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n_ \u003d (wikipedia_clean\n.select(\"categoria\",length(\"documents\").alias(\u0027documents_len\u0027))\n.groupBy(\"categoria\")\n.agg(\n    max(\"documents_len\").alias(\"max_documents_len\"),\n    min(\"documents_len\").alias(\"min_documents_len\"),\n    avg(\"documents_len\").alias(\"avg_documents_len\")\n)\n.sort(desc(\"avg_documents_len\"))\n.createOrReplaceTempView(\"wikipedia_e2\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT * FROM wikipedia_e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 3. Creare **nuvole di parole** per ogni categoria (basate su `documents`)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Creo il dataframe da usare nei grafici wordclouds**"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# **stabilisco le variabili per effettuare le trasformazioni necessarie al fine di creare grafici di tipo word cloud**\nLANG   \u003d \"english\"\nTOP_K  \u003d 50\nMIN_DF \u003d 2\nBASE_SW     \u003d StopWordsRemover.loadDefaultStopWords(LANG)\nNUM_WORDS   \u003d [\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\n               \"ten\",\"eleven\",\"twelve\",\"thirteen\",\"fourteen\",\"fifteen\",\"sixteen\",\n               \"seventeen\",\"eighteen\",\"nineteen\",\"twenty\"]\nHIGH_DF_SW  \u003d [\"new\",\"also\",\"first\",\"second\",\"one\",\"two\",\"three\",\"later\"]\nCUSTOM_SW   \u003d [\"’s\",\"“\",\"”\",\"—\",\"http\",\"https\",\"\"]\nstopwords   \u003d BASE_SW + NUM_WORDS + HIGH_DF_SW + CUSTOM_SW\n\n# **Divido in Tokens la colonna documents**\ntokenizer \u003d Tokenizer(inputCol\u003d\"documents\", outputCol\u003d\"document_tokens\")\ndf_tokens \u003d tokenizer.transform(wikipedia_clean)\n\n# **Rimuovo le Stop Words**\nremover \u003d StopWordsRemover(\n    inputCol\u003d\"document_tokens\",\n    outputCol\u003d\"clean_tokens\",\n    stopWords\u003dstopwords,\n    caseSensitive\u003dFalse\n)\n\ndf_no_sw \u003d remover.transform(df_tokens)\ndf_clean \u003d df_no_sw.select(\"categoria\", \"clean_tokens\").repartition(\"categoria\")\n\n# **Conteggio token per categoria**\ndf_counts \u003d (\n    df_clean\n    .select(\"categoria\", explode(\"clean_tokens\").alias(\"token\"))\n    .groupBy(\"categoria\", \"token\")\n    .count()\n    .where(col(\"count\") \u003e\u003d MIN_DF)\n)\n\n# **Effettuo il ranking dei token tramite windows function**\n# Effettuo il ranking e filtro per `TOP_K`, successivamente raggruppo per categoria e uso `map_from_entries` per creare la colonna `freqs` che sarà un dizionario contenente come chiave la parola e come valore il \n# conteggio di quante volte la parola compare in quella categoria.\nw \u003d Window.partitionBy(\"categoria\").orderBy(desc(\"count\"), asc(\"token\"))\n\ndf_wordcloud \u003d (\n    df_counts\n    .withColumn(\"rank\", row_number().over(w))\n    .where(col(\"rank\") \u003c\u003d TOP_K)\n    .groupBy(\"categoria\")\n    .agg(\n        map_from_entries(\n            collect_list(struct(\"token\", \"count\"))\n        ).alias(\"freqs\")\n    )\n)\n\n# **Mostro e salvo il risultato finale**\ndf_wordcloud.createOrReplaceTempView(\"df_wordcloud\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * from df_wordcloud"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Creo i grafici wordcloud**"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# **Converto Dataframe da Spark a Pandas**\n# Per agevolarmi nella creazione dei grafici converto il Dataframe da Spark a Pandas.\ndf_wordcloud_pandas \u003d df_wordcloud.sort(\"categoria\").toPandas()\n\n# **Calcolo le dimensioni ottimali della griglia in base al numero di categorie**\nn_cat  \u003d len(df_wordcloud_pandas)\nn_cols \u003d 5\nn_rows \u003d math.ceil(n_cat / n_cols)\n\n# **Creazione grafici**\n# Creo la figura e gli `n_rows * n_cols` subplot, impostando dinamicamente la grandezza della figura in pollici \n# (ho fatto vari tentativi).\nfig, axes \u003d plt.subplots(\n    n_rows, n_cols,\n    figsize\u003d(n_cols * 8, n_rows * 6),\n)\n\naxes \u003d axes.flatten()\nfor idx, (_, row) in enumerate(df_wordcloud_pandas.iterrows()):\n    ax \u003d axes[idx]\n    freqs \u003d {token: int(count) for token, count in row[\"freqs\"].items()}\n    wc \u003d WordCloud(\n        width\u003d400, height\u003d200,\n        background_color\u003d\"white\",\n        colormap\u003d\"tab10\",\n        prefer_horizontal\u003d1.0\n    ).generate_from_frequencies(freqs)\n    ax.imshow(wc, interpolation\u003d\"bilinear\")\n    ax.set_title(row[\"categoria\"], fontsize\u003d20, pad\u003d10)\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **M – Model (Modellazione e machine learning)**\n\n### Obiettivo:\n\nAddestrare un classificatore per prevedere la categoria degli articoli."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Concatenare le due colonne testuali\nwikipedia_m0 \u003d wikipedia_clean.withColumn(\"content\", concat_ws(\" \", wikipedia_clean[\"summary\"], wikipedia_clean[\"documents\"])).drop(\"title\",\"summary\",\"documents\").persist(StorageLevel.DISK_ONLY)\nwikipedia_m0.count()\n\n# Preprocessing e pipeline\nlabel_indexer \u003d StringIndexer(inputCol\u003d\"categoria\", outputCol\u003d\"label\")\ntokenizer \u003d Tokenizer(inputCol\u003d\"content\", outputCol\u003d\"tokens\")\nremover \u003d StopWordsRemover(inputCol\u003d\"tokens\", outputCol\u003d\"filtered\")\nvectorizer \u003d CountVectorizer(inputCol\u003d\"tokens\", outputCol\u003d\"counts\",vocabSize \u003d 10000)\nscaler1 \u003d StandardScaler(inputCol\u003d\"counts\", outputCol\u003d\"scaled_counts\")\n\nlr \u003d LogisticRegression(featuresCol\u003d\"scaled_counts\", labelCol\u003d\"label\")\n\npipeline \u003d Pipeline(stages\u003d[\nlabel_indexer,\ntokenizer,\nremover,\nvectorizer,\nscaler1,\nlr\n])\n\n# Addestramento\ntrain_data, test_data \u003d wikipedia_m0.randomSplit([0.8, 0.2], seed\u003d42)\nmodel \u003d pipeline.fit(train_data)\n\n# Valutazione\npredictions \u003d model.transform(test_data)\n\nevaluator \u003d MulticlassClassificationEvaluator(labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"accuracy\")\naccuracy \u003d evaluator.evaluate(predictions)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nprecision_evaluator \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"weightedPrecision\")\nprecision \u003d precision_evaluator.evaluate(predictions)\nprint(f\"Weighted Precision: {precision:.4f}\")\n\nrecall_evaluator \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"weightedRecall\")\nrecall \u003d recall_evaluator.evaluate(predictions)\nprint(f\"Weighted Recall: {recall:.4f}\")\n\nf1_evaluator \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"f1\")\nf1 \u003d f1_evaluator.evaluate(predictions)\nprint(f\"Weighted F1 Score: {f1:.4f}\")\n\nprediction_and_labels \u003d predictions.select(\"prediction\", \"label\").rdd.map(tuple)\nmetrics \u003d MulticlassMetrics(prediction_and_labels)\nlabel_indexer_model \u003d label_indexer.fit(wikipedia_m0)\nlabels \u003d label_indexer_model.labels\ncm \u003d metrics.confusionMatrix().toArray()\n\nplt.figure(figsize\u003d(12,10))\nsns.heatmap(cm, annot\u003dTrue, fmt\u003d\u0027g\u0027, cmap\u003d\u0027Blues\u0027, xticklabels\u003dlabels, yticklabels\u003dlabels)\nplt.xlabel(\u0027Predetto\u0027)\nplt.ylabel(\u0027Reale\u0027)\nplt.title(\u0027Confusion Matrix\u0027)\nplt.xticks(rotation\u003d45, ha\u003d\u0027right\u0027)\nplt.yticks(rotation\u003d0)\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Recupero gli stadi della pipeline\nvec   \u003d model.stages[3] # CountVectorizerModel\nlr_md \u003d model.stages[-1] # LogisticRegressionModel\n\nvocab  \u003d vec.vocabulary # lista dei token\ncoefs  \u003d lr_md.coefficientMatrix.toArray() # shape \u003d (nClassi, vocabSize)\n\nimport numpy as np, pandas as pd\n\nglobal_imp \u003d np.abs(coefs).mean(axis\u003d0)\ntop20_idx  \u003d global_imp.argsort()[-20:][::-1]\n\ntop20_dfp \u003d pd.DataFrame({\n    \"token\": [vocab[i] for i in top20_idx],\n    \"peso\" : global_imp[top20_idx]\n})\n\ntop20_df \u003d spark.createDataFrame(top20_dfp)\n\ntop20_df.createOrReplaceTempView(\"top20_df\")"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * from top20_df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n## Risultati del modello di classificazione\r\n\r\n\u003e *Disclaimer*: Il modello attuale è **molto semplice e limitato** perché sto lavorando in ambiente **locale** con **risorse ridotte**.  \r\n\u003e L\u0027obiettivo è ottenere una baseline funzionante, non ottimizzare al massimo le prestazioni.\r\n\r\n- **Accuracy**: 0.8491  \r\n- **Weighted Precision**: 0.8505  \r\n- **Weighted Recall**: 0.8491  \r\n- **Weighted F1 Score**: 0.8497  \r\n\r\n### Considerazioni\r\n- Le metriche sono **coerenti** e indicano un **modello bilanciato**.  \r\n- La **confusion matrix** conferma che la maggior parte delle classi è ben gestita, con solo lievi confusioni tra classi affini.\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n## **N – iNterpret (Interpretazione dei risultati)**\r\n\r\n### Obiettivo:\r\n\r\nFornire una lettura chiara e strategica dei risultati ottenuti.\r\n\r\n### Task:\r\n\r\n### 1. Sintesi dei risultati dell’EDA\r\n\r\n* **Distribuzione delle categorie**\r\n\r\n  * Gli articoli non sono equamente distribuiti.\r\n  * `medicine` domina con **8.311 articoli**.\r\n  * `politics`, `culture` e `sports` contano meno della metà di `medicine`.\r\n  * *Rischio*: i modelli potrebbero favorire le classi più frequenti.\r\n\r\n* **Lunghezza dei testi per categoria**\r\n\r\n  * `politics` contiene in media i testi più lunghi, mentre `pets` ha lunghezze medie basse.\r\n  * Tutte le categorie presentano lunghezza minima e massima molto distanti, il che è un forte segnale di varianza interna.\r\n  * *Effetto sui modelli*: sequenze molto eterogenee rendono l\u0027apprendimento instabile e penalizzano le categorie con meno contenuto.\r\n\r\n* **Word‑Cloud per categoria**\r\n\r\n  * Buona coerenza lessicale in `medicine`, `research`, `energy`, `transport`, `politics`.\r\n  * Rumore/ambiguità:\r\n\r\n    * `trade` contiene termini geografici‑storici.\r\n    * `culture` mostra parole legate a rituali funebri (*cemetery*).\r\n    * `engineering` è contaminata da contenuti geografico‑culturali.\r\n  * Alcune categorie ignorano concetti chiave: in `technology` prevalgono *games* a scapito di *AI*, *IoT*, *cloud*.\r\n\r\n### 2. Analisi dei 20 token più importanti in relazione all\u0027EDA\r\n \r\n* **Bias sanitario**: 4 token su 20 (hospital, medical, medicine, research) appartengono a `medicine`, accentuando lo sbilanciamento già evidenziato nella distribuzione delle categorie.\r\n\r\n* **Contaminazione geografica**: *dresden* e *saxony* occupano posizioni alte; documenti storici‑locali penetrano categorie economiche (`trade`) o ingegneristiche.\r\n\r\n* **Ambiguità lessicale**: token come *polo*, *bridge* e *station* mostrano la sovrapposizione di significati (sport vs. moda, infrastruttura vs. monumento).\r\n\r\n### 3. Raccomandazioni per l\u0027organizzazione dei contenuti su Wikipedia\r\n\r\n* **Disambiguazione geografica**: Introdurre un tag dove collocare voci come `dresden`, `saxony`, `bridge` (monumenti storici) e `cemetery`. \r\n\r\n* **Uniformità nella lunghezza dei contenuti**: Introdurre linee guida editoriali per garantire una lunghezza coerente tra articoli della stessa categoria.\r\n\r\n* **Revisione semantica delle categorie**: Eseguire un audit semantico per ricollocare articoli fuori tema: ad esempio, spostare contenuti su games fuori da technology se non trattano \r\ninnovazione tecnologica, o separare rituali funebri dalla categoria culture. Questo ridurrebbe l\u0027ambiguità lessicale e aumenterebbe la coerenza semantica delle categorie."
    }
  ]
}